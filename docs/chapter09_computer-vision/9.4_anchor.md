```python
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import math
%matplotlib inline
```

# 9.4 锚框

目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边缘从而更准确地预测目标的真实边界框（ground-truth bounding box）。不同的模型使用的区域采样方法可能不同。这里我们介绍其中的一种方法：它以每个像素为中心生成多个大小和宽高比（aspect ratio）不同的边界框。这些边界框被称为锚框（anchor box）。我们将在“单发多框检测（SSD）”一节基于锚框实践目标检测。

## 9.4.1. 生成多个锚框

假设输入图像高为 h ，宽为 w 。我们分别以图像的每个像素为中心生成不同形状的锚框。设大小为 s∈(0,1] 且宽高比为 r>0 ，那么锚框的宽和高将分别为 wsr√ 和 hs/r√ 。当中心位置给定时，已知宽和高的锚框是确定的。

下面我们分别设定好一组大小 s1,…,sn 和一组宽高比 r1,…,rm 。如果以每个像素为中心时使用所有的大小与宽高比的组合，输入图像将一共得到 whnm 个锚框。虽然这些锚框可能覆盖了所有的真实边界框，但计算复杂度容易过高。因此，我们通常只对包含 s1 或 r1 的大小与宽高比的组合感兴趣，即

(s1,r1),(s1,r2),…,(s1,rm),(s2,r1),(s3,r1),…,(sn,r1).

也就是说，以相同像素为中心的锚框的数量为 n+m−1 。对于整个输入图像，我们将一共生成 wh(n+m−1) 个锚框。

以上生成锚框的方法已实现在MultiBoxPrior函数中。指定输入、一组大小和一组宽高比，该函数将返回输入的所有锚框。


```python
img_raw = tf.io.read_file('catdog.jpg')
img = tf.image.decode_jpeg(img_raw).numpy()
h, w = img.shape[0:2]
print(h, w)

def MultiBoxPrior(feature_map, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5]):
    """
    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成(xmin, ymin, xmax, ymax).
    https://zh.d2l.ai/chapter_computer-vision/anchor.html
    Args:
        feature_map: torch tensor, Shape: [N, C, H, W].
        sizes: List of sizes (0~1) of generated MultiBoxPriores. 
        ratios: List of aspect ratios (non-negative) of generated MultiBoxPriores. 
    Returns:
        anchors of shape (1, num_anchors, 4). 由于batch里每个都一样, 所以第一维为1
    """
    pairs = [] # pair of (size, sqrt(ratio))
    for r in ratios:
        pairs.append([sizes[0], np.sqrt(r)])
    for s in sizes[1:]:
        pairs.append([s, np.sqrt(ratios[0])])
    
    pairs = np.array(pairs)

    ss1 = pairs[:, 0] * pairs[:, 1] # size * sqrt(ration)
    ss2 = pairs[:, 0] / pairs[:, 1] # size / sqrt(retion)

    base_anchors = tf.stack([-ss1, -ss2, ss1, ss2], axis=1) / 2

    h, w = feature_map.shape[-2:]
    shifts_x = tf.divide(tf.range(0, w), w)
    shifts_y = tf.divide(tf.range(0, h), h)
    shift_x, shift_y = tf.meshgrid(shifts_x, shifts_y)
    shift_x = tf.reshape(shift_x, (-1,))
    shift_y = tf.reshape(shift_y, (-1,))
    shifts = tf.stack((shift_x, shift_y, shift_x, shift_y), axis=1)

    anchors = tf.add(tf.reshape(shifts, (-1,1,4)), tf.reshape(base_anchors, (1,-1,4)))
    return tf.cast(tf.reshape(anchors, (1,-1,4)), tf.float32)

x = tf.zeros((1,3,h,w))
y = MultiBoxPrior(x)
y.shape
```

    252 322
    




    TensorShape([1, 405720, 4])



我们看到，返回锚框变量y的形状为（1，锚框个数，4）。将锚框变量y的形状变为（图像高，图像宽，以相同像素为中心的锚框个数，4）后，我们就可以通过指定像素位置来获取所有以该像素为中心的锚框了。下面的例子里我们访问以（250，250）为中心的第一个锚框。它有4个元素，分别是锚框左上角的xx和yy轴坐标和右下角的xx和yy轴坐标，其中xx和yy轴的坐标值分别已除以图像的宽和高，因此值域均为0和1之间。


```python
boxes = tf.reshape(y, (h,w,5,4))
boxes[250,250,0,:]
```




    <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.40139753, 0.61706346, 1.1513975 , 1.3670635 ], dtype=float32)>



可以验证一下以上输出对不对：size和ratio分别为0.75和1, 则(归一化后的)宽高均为0.75, 所以输出是正确的（0.75 = 0.75378788 - 0.00378788 = 1.04885445 - 0.29885445）。

为了描绘图像中以某个像素为中心的所有锚框，我们先定义show_bboxes函数以便在图像上画出多个边界框。


```python
def bbox_to_rect(bbox, color):
    # 将边界框(左上x, 左上y, 右下x, 右下y)格式转换成matplotlib格式：
    # ((左上x, 左上y), 宽, 高)
    return plt.Rectangle(
        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],
        fill=False, edgecolor=color, linewidth=2)
```


```python
# 本函数已保存在d2lzh_pytorch包中方便以后使用
def show_bboxes(axes, bboxes, labels=None, colors=None):
    def _make_list(obj, default_values=None):
        if obj is None:
            obj = default_values
        elif not isinstance(obj, (list, tuple)):
            obj = [obj]
        return obj
    
    labels = _make_list(labels)
    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])
    for i, bbox in enumerate(bboxes):
        color = colors[i % len(colors)]
        rect = bbox_to_rect(bbox.numpy(), color)
        axes.add_patch(rect)
        if labels and len(labels) > i:
            text_color = 'k' if color == 'w' else 'w'
            axes.text(rect.xy[0], rect.xy[1], labels[i],
                va='center', ha='center', fontsize=6,
                color=text_color, bbox=dict(facecolor=color, lw=0))

```

刚刚我们看到，变量boxes中xx和yy轴的坐标值分别已除以图像的宽和高。在绘图时，我们需要恢复锚框的原始坐标值，并因此定义了变量bbox_scale。现在，我们可以画出图像中以(250, 250)为中心的所有锚框了。可以看到，大小为0.75且宽高比为1的锚框较好地覆盖了图像中的狗。


```python
from IPython import display
def use_svg_display():
    """Use svg format to display plot in jupyter"""
    display.set_matplotlib_formats('svg')

use_svg_display()
# 设置图的尺寸
plt.rcParams['figure.figsize'] = (3.5, 2.5)


fig = plt.imshow(img)
bbox_scale = tf.constant([[w,h,w,h]], dtype=tf.float32)
show_bboxes(fig.axes, tf.multiply(boxes[200,250,:,:], bbox_scale), 
    ['s=0.75, r=1', 's=0.75, r=2', 's=0.55, r=0.5', 
     's=0.5, r=1', 's=0.25, r=1'])
```


![svg](../img/chapter09/9.4/output_17_0.svg)


## 9.4.2 交并比

我们刚刚提到某个锚框较好地覆盖了图像中的狗。如果该目标的真实边界框已知，这里的“较好”该如何量化呢？一种直观的方法是衡量锚框和真实边界框之间的相似度。我们知道，Jaccard系数（Jaccard index）可以衡量两个集合的相似度。给定集合AA和BB，它们的Jaccard系数即二者交集大小除以二者并集大小：

![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAJUAAAAnCAYAAAAGoAoxAAAIhklEQVR4Ae2ah28VRxCH+YOiKBKRkkiJEiGlkwRESAihBnAooReHDgETBAEhmumhit57xzGY3jummIDBdGOMscHxRN+iPdb37r239+6ejdGN9PT29vZ2dmd+OzM7u/UkokgCIUugXsj9Rd1FEpAIVBEIQpdABKrQRRp1GIEqwkDoEohAFbpIU+tww648KXtWntrHAb8Km3cEqoAKCevzNdv/SQqqazduSbv+o+XC1evWbMsrnsuidduk7+ipUlh0T7Ky58u7jdvLhz92lrxjp1U/NrytGYpEgbofYaWzbTLFAo5+Y7Ll4+ZdHVBVVVXJ8XOXZPikv2XMjEUyevpCyZo6X/69dccZ6snzl2XIhNkKjKOyF8idB4+kpPSpZAwaIzxDyXg7nVkWIktlKah0N0um2NzDJ2XC3GXSMCPTAdXeo6eEesClCfAtWLNViu49UFW8GzZxjnzZro+yVFS+eFEpgyfMksOnzqs2yXjrvm3/awRU+QU3ZNritcKEXzfauDtP1u3cW00xtTHGRIp9+LhE/pq1WFklDarKykplYSqev4gZLu5x+77Dqv5JaZm07pcls5dvUHMsuHlbgWzf0dPOnBPxjuncoiLtoGLFYLbvPnhkMZxXTQ4cPyuNOvV3VuWrN8lLrMStuYekUacBkl9wU0rLnsnURaulQYtu8tZXraRp1yEC0CGUgwXYc/B48o7T2CKeYrE005esFdwYYNGgIqjnGy+inX5XUFik5os8W/XNUvNv3HmA8KwtXDzeXn3b1KUMqvKKCoV4Jvl+01/l7YZt5LM2vWTm0vUOX5SL6c05dMKpsykglI+adVE/yn7o8ZNS6Th4rNRv1F6+ychUwWjznsNlzoqNCmhtMkcpwRJnACjozv2H0mXYeMc9+OEXVtt4is09ckrmrtz00soUFik3duT0BRXUa+C4xwCQ2NFByL7T0HHOJgD5oJPP2/aW64VFqk083u5+bZ9TBpVmgGIGjJsu33XsL/ceFutq9X/m0lU1IUywLQFEAs9UQfVfVZXgLhDkJz93laZdB8u5ywUO+8I79+SLX3qrMWtQ8RL3jMXSq9f5oIYKXorFyn//2yC1CLCw+kcslchS3X9ULNv2vnR/4+csVa7PnAYL9YduQwRZQF68zfZ+y4FBBZAAFMAylYRyxs1eopTlZ1AEj4CKQBJg+bVU8EKoWCmUsHDttmrsr964pVY7FsCkkxcuK/fwsLjErK6xslYsQAI0zGHElLmC1dGk4yMWgA2omAuWGWBduvbS3RPXzlq2XhZv2OEsIM1b8wn6HxhUmOL6jdrJmh251cZSXFIqP/UYpnx3tRcJHjDNfUZPEfIxf0yemzKotPskdjJBQpBKnmfl1j2OQPVwUGKTLoOE+dQGacWSDsCK4MJv3325gzPHM2n+CmnQsrty5yu35JivnLK2VJty9svEeStUSuGdb9vK1x36qcWPHEzSvM26IOXAoJq3eou81yRDWOkmESBjwWwtDZaNrTArCGJ1kaA7f8VfTMW3xBFYKXY8uEOA0jNroooj9DbaHCtlVj4u07043O3S9exXscjrWXmF53CYMzGvLfnlnazfQKBiO9tr1GRp1mOoYJlMAmTEM7a7vnOXr8nAcTOcgBK3BTBwBX4J94DrPHE+X2WSdSzCPzsfvfMz+0VJBLAsktqgsBXrZw5h8w4EKh30jpwyL8adAAbiGkxxMnrytEzowzT3qYJKAx03R78QgOGIgqMKgNW813DB1boJl8svEQE6LJrt79TFK4m6c96FrVinY4tC2LwDgYpgl1QCOSE32YKK3R7HC6Y1McvuQNvNx/2MZSR2wH26CSARuMbbANiAyt1nGM/mfGu7HMZ8rEGF8k9fvFrNzaE4zqKIn9xEHEPy0e3+sBrPX7zKAu/af1QdctIOq6Z/m3MOKKB5gcPNy3yGL3mzgyfPmdWqrF2cl7vW72rL/cUMtg5XWIOKoJdVxMElpLe3ppsx5RAvUOcAlNiFQBJA4Yr0OZX5vd7BEbOZRxGUzWfzG4BBGoNdH4k9FoJJuFfyPhzLuMk2UE+X+wv7+ol7fome4/Em4H9a9izRp57vrEGFxeBkmy0vyuM6BVtbM7FocoiXUliyYafj6jg2ifc9qQAs3QdNO0rvPyerLTRJTdIUgMZtAeF96+595fpow/WOln1GqOMN2u7IOyIteo8QLCDjdxMWkq28exfrbpeuZ5u4piavviAjksHIEUPgh6xB9eDRY3WGlzk2W+V6ULQ732EyZlAE3+zETMIqteo7Uj5t3VOOnrlovqpWJpFKEhRXRrKOpB0xEYej1Hnlk1ZtzVFJUxKGA8fPFHIzxHykNujDKzjXTAET8ZZ7F6vfp/s/GaiYv3n1hXSCl8VlnCwQvAAU5OqLOi9t2d1JnNrKwBpUth2a7cgJdRg4RrlKsz5oGWsTtkUB/Lj42qJkoHJffbHJqDMXFjfhxhtz9YXV9fvYab4PlBMpljhpxpJ11TLlidrbvHudD5QZv9fVF1tQ6dg3uvqSAAlky1ds2ZOghb9XuNk37eoL7k8fKNepqy9+VBfWJT1MOXGce1fnZyzutq/7Jb1Urr6YoKqTV1/cSoqeU5OAV0yV6OoLgfryzbur3QzRnEnn7Mw7oh7ZtbtjRdI17HRf26sveiLRfzAJaFDZXn2BG99w/OQmbrGezb+m4s46efXFPaHoOTUJaFDZXn3hRgdBOFaIVMqN23dVzm/KwlWy+8Axteurs1dfUhNh9JVbAhpU7nqbZ3aCxFEkjFOJN4Pw9hpfWvNUXgyjOm8JhK1Yby7etWHzjkDlLecarw1bsX4mEDbvCFR+pJ/Gttz1N+/4p5FVTNdh845AFSPiqCKoBCJQBZVg9H2MBCJQxYgkqggqgQhUQSUYfR8jgQhUMSKJKoJK4H84mGT6N3DodwAAAABJRU5ErkJggg==)

实际上，我们可以把边界框内的像素区域看成是像素的集合。如此一来，我们可以用两个边界框的像素集合的Jaccard系数衡量这两个边界框的相似度。当衡量两个边界框的相似度时，我们通常将Jaccard系数称为交并比（Intersection over Union，IoU），即两个边界框相交面积与相并面积之比，如图9.2所示。交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框相等。

![alt text](http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter09/9.4_iou.svg)

下面我们对其进行实现。


```python
set_1 = [[1,2,3,4],[5,6,7,8]]
set_2 = [[1,1,1,1],[2,2,2,2]]
lower_bounds = tf.maximum(tf.expand_dims(set_1, axis=1), tf.expand_dims(set_2, axis=0)) # (n1, n2, 2)
upper_bounds = tf.minimum(tf.expand_dims(set_1, axis=1), tf.expand_dims(set_2, axis=0)) # (n1, n2, 2)

tf.expand_dims(set_1, axis=1), tf.expand_dims(set_2, axis=0), lower_bounds, tf.multiply(set_1, set_2), tf.subtract(set_1, set_2)
```




    (<tf.Tensor: shape=(2, 1, 4), dtype=int32, numpy=
     array([[[1, 2, 3, 4]],
     
            [[5, 6, 7, 8]]])>,
     <tf.Tensor: shape=(1, 2, 4), dtype=int32, numpy=
     array([[[1, 1, 1, 1],
             [2, 2, 2, 2]]])>,
     <tf.Tensor: shape=(2, 2, 4), dtype=int32, numpy=
     array([[[1, 2, 3, 4],
             [2, 2, 3, 4]],
     
            [[5, 6, 7, 8],
             [5, 6, 7, 8]]])>,
     <tf.Tensor: shape=(2, 4), dtype=int32, numpy=
     array([[ 1,  2,  3,  4],
            [10, 12, 14, 16]])>,
     <tf.Tensor: shape=(2, 4), dtype=int32, numpy=
     array([[0, 1, 2, 3],
            [3, 4, 5, 6]])>)




```python
# 以下函数已保存在d2lzh_pytorch包中方便以后使用
# 参考https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py#L356
def compute_intersection(set_1, set_2):
    """
    计算anchor之间的交集
    Args:
        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)
        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)
    Returns:
        intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)
    """
    # tensorflow auto-broadcasts singleton dimensions
    lower_bounds = tf.maximum(tf.expand_dims(set_1[:,:2], axis=1), tf.expand_dims(set_2[:,:2], axis=0)) # (n1, n2, 2)
    upper_bounds = tf.minimum(tf.expand_dims(set_1[:,2:], axis=1), tf.expand_dims(set_2[:,2:], axis=0)) # (n1, n2, 2)
    # 设置最小值
    intersection_dims = tf.clip_by_value(upper_bounds - lower_bounds, clip_value_min=0, clip_value_max=3) # (n1, n2, 2)
    return tf.multiply(intersection_dims[:, :, 0], intersection_dims[:, :, 1]) # (n1, n2)

def compute_jaccard(set_1, set_2):
    """
    计算anchor之间的Jaccard系数(IoU)
    Args:
        set_1: a tensor of dimensions (n1, 4), anchor表示成(xmin, ymin, xmax, ymax)
        set_2: a tensor of dimensions (n2, 4), anchor表示成(xmin, ymin, xmax, ymax)
    Returns:
        Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, shape: (n1, n2)
    """
    # Find intersections
    intersection = compute_intersection(set_1, set_2)

    # Find areas of each box in both sets
    areas_set_1 = tf.multiply(tf.subtract(set_1[:, 2], set_1[:, 0]), tf.subtract(set_1[:, 3], set_1[:, 1]))  # (n1)
    areas_set_2 = tf.multiply(tf.subtract(set_2[:, 2], set_2[:, 0]), tf.subtract(set_2[:, 3], set_2[:, 1]))  # (n2)

    # Find the union
    union = tf.add(tf.expand_dims(areas_set_1, axis=1), tf.expand_dims(areas_set_2, axis=0))  # (n1, n2)
    union = tf.subtract(union, intersection)  # (n1, n2)

    return tf.divide(intersection, union) #(n1, n2)
```


```python
tf.expand_dims(boxes[200,250,:,:][:, :2], axis=1), tf.expand_dims(boxes[210,260,1:2,:][:, :2], axis=0)
```




    (<tf.Tensor: shape=(5, 1, 2), dtype=float32, numpy=
     array([[[0.40139753, 0.4186508 ]],
     
            [[0.24606743, 0.5284858 ]],
     
            [[0.5112325 , 0.2633207 ]],
     
            [[0.5263975 , 0.5436508 ]],
     
            [[0.6513975 , 0.6686508 ]]], dtype=float32)>,
     <tf.Tensor: shape=(1, 1, 2), dtype=float32, numpy=array([[[0.27712333, 0.5681683 ]]], dtype=float32)>)




```python
tf.maximum(tf.expand_dims(boxes[200,250,:,:][:, :2], axis=1), tf.expand_dims(boxes[210,260,1:2,:][:, :2], axis=0))
```




    <tf.Tensor: shape=(5, 1, 2), dtype=float32, numpy=
    array([[[0.40139753, 0.5681683 ]],
    
           [[0.27712333, 0.5681683 ]],
    
           [[0.5112325 , 0.5681683 ]],
    
           [[0.5263975 , 0.5681683 ]],
    
           [[0.6513975 , 0.6686508 ]]], dtype=float32)>



在本节的剩余部分，我们将使用交并比来衡量锚框与真实边界框以及锚框与锚框之间的相似度。

## 9.4.3 标注训练集的锚框

在训练集中，我们将每个锚框视为一个训练样本。为了训练目标检测模型，我们需要为每个锚框标注两类标签：一是锚框所含目标的类别，简称类别；二是真实边界框相对锚框的偏移量，简称偏移量（offset）。在目标检测时，我们首先生成多个锚框，然后为每个锚框预测类别以及偏移量，接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框。

我们知道，在目标检测的训练集中，每个图像已标注了真实边界框的位置以及所含目标的类别。在生成锚框之后，我们主要依据与锚框相似的真实边界框的位置和类别信息为锚框标注。那么，该如何为锚框分配与其相似的真实边界框呢？

假设图像中锚框分别为 A1,A2,…,Ana ，真实边界框分别为 B1,B2,…,Bnb ，且 na≥nb 。定义矩阵 X∈Rna×nb ，其中第 i 行第 j 列的元素 xij 为锚框 Ai 与真实边界框 Bj 的交并比。 首先，我们找出矩阵 X 中最大元素，并将该元素的行索引与列索引分别记为 i1,j1 。我们为锚框 Ai1 分配真实边界框 Bj1 。显然，锚框 Ai1 和真实边界框 Bj1 在所有的“锚框—真实边界框”的配对中相似度最高。接下来，将矩阵 X 中第 i1 行和第 j1 列上的所有元素丢弃。找出矩阵 X 中剩余的最大元素，并将该元素的行索引与列索引分别记为 i2,j2 。我们为锚框 Ai2 分配真实边界框 Bj2 ，再将矩阵 X 中第 i2 行和第 j2 列上的所有元素丢弃。此时矩阵 X 中已有2行2列的元素被丢弃。 依此类推，直到矩阵 X 中所有 nb 列元素全部被丢弃。这个时候，我们已为 nb 个锚框各分配了一个真实边界框。 接下来，我们只遍历剩余的 na−nb 个锚框：给定其中的锚框 Ai ，根据矩阵 X 的第 i 行找到与 Ai 交并比最大的真实边界框 Bj ，且只有当该交并比大于预先设定的阈值时，才为锚框 Ai 分配真实边界框 Bj 。

如图9.3（左）所示，假设矩阵 X 中最大值为 x23 ，我们将为锚框 A2 分配真实边界框 B3 。然后，丢弃矩阵中第2行和第3列的所有元素，找出剩余阴影部分的最大元素 x71 ，为锚框 A7 分配真实边界框 B1 。接着如图9.3（中）所示，丢弃矩阵中第7行和第1列的所有元素，找出剩余阴影部分的最大元素 x54 ，为锚框 A5 分配真实边界框 B4 。最后如图9.3（右）所示，丢弃矩阵中第5行和第4列的所有元素，找出剩余阴影部分的最大元素 x92 ，为锚框 A9 分配真实边界框 B2 。之后，我们只需遍历除去 A2,A5,A7,A9 的剩余锚框，并根据阈值判断是否为剩余锚框分配真实边界框。

![alt text](https://zh.d2l.ai/_images/anchor-label.svg)

现在我们可以标注锚框的类别和偏移量了。如果一个锚框 A 被分配了真实边界框 B ，将锚框 A 的类别设为 B 的类别，并根据 B 和 A 的中心坐标的相对位置以及两个框的相对大小为锚框 A 标注偏移量。由于数据集中各个框的位置和大小各异，因此这些相对位置和相对大小通常需要一些特殊变换，才能使偏移量的分布更均匀从而更容易拟合。设锚框 A 及其被分配的真实边界框 B 的中心坐标分别为 (xa,ya) 和 (xb,yb) ， A 和 B 的宽分别为 wa 和 wb ，高分别为 ha 和 hb ，一个常用的技巧是将 A 的偏移量标注为

![image.png](data:image/png;base64,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)

其中常数的默认值为 μx=μy=μw=μh=0,σx=σy=0.1,σw=σh=0.2 。如果一个锚框没有被分配真实边界框，我们只需将该锚框的类别设为背景。类别为背景的锚框通常被称为负类锚框，其余则被称为正类锚框。

下面演示一个具体的例子。我们为读取的图像中的猫和狗定义真实边界框，其中第一个元素为类别（0为狗，1为猫），剩余4个元素分别为左上角的 x 和 y 轴坐标以及右下角的 x 和 y 轴坐标（值域在0到1之间）。这里通过左上角和右下角的坐标构造了5个需要标注的锚框，分别记为 A0,…,A4 （程序中索引从0开始）。先画出这些锚框与真实边界框在图像中的位置。


```python
bbox_scale = tf.constant([[w,h,w,h]], dtype=tf.float32)
ground_truth = tf.constant([[0, 0.13, 0, 0.47, 1],
                [1, 0.47, 0.15, 0.84, 1]])
anchors = tf.constant([[0, 0.1, 0.2, 0.3],
            [0.15, 0.2, 0.4, 0.4],
            [0.63, 0.05, 0.88, 0.98],
            [0.66, 0.45, 0.8, 0.8],
            [0.57, 0.3,  0.92, 0.9]])

fig = plt.imshow(img)
show_bboxes(fig.axes, tf.multiply(ground_truth[:, 1:], bbox_scale),
        ['dog', 'cat'], 'k')
show_bboxes(fig.axes, tf.multiply(anchors, bbox_scale),
        ['0', '1', '2', '3', '4'])
```


![svg](../img/chapter09/9.4/output_38_0.svg)


下面实现MultiBoxTarget函数来为锚框标注类别和偏移量。该函数将背景类别设为0，并令从零开始的目标类别的整数索引自加1（1为狗，2为猫）。


```python
def assign_anchor(bb, anchor, jaccard_threshold=0.5):
    """
    # 按照「9.4.1. 生成多个锚框」图9.3所讲为每个anchor分配真实的bb, anchor表示成归一化(xmin, ymin, xmax, ymax).
    https://zh.d2l.ai/chapter_computer-vision/anchor.html
    Args:
        bb: 真实边界框(bounding box), shape:（nb, 4）
        anchor: 待分配的anchor, shape:（na, 4）
        jaccard_threshold: 预先设定的阈值
    Returns:
        assigned_idx: shape: (na, ), 每个anchor分配的真实bb对应的索引, 若未分配任何bb则为-1
    """
    na = anchor.shape[0]
    nb = bb.shape[0]
    jaccard = compute_jaccard(anchor, bb).numpy()   # shape: (na, nb)
    assigned_idx = np.ones(na) * -1 # 初始全为-1

    # 先为每个bb分配一个anchor（不要求满足jaccard_threshold）
    jaccard_cp = jaccard.copy()
    for j in range(nb):
        i = np.argmax(jaccard_cp[:, j])
        assigned_idx[i] = j
        jaccard_cp[i, :] = float("-inf")    # 赋值为负无穷, 相当于去掉这一行
    
    # 处理还未被分配的anchor， 要求满足jaccard_threshold
    for i in range(na):
        if assigned_idx[i] == -1:
            j = np.argmax(jaccard[i, :])
            if jaccard[i, j] >= jaccard_threshold:
                assigned_idx[i] = j
    return tf.cast(assigned_idx, tf.int32)

def xy_to_cxcy(xy):
    """
    将(x_min, y_min, x_max, y_max)形式的anchor转换成(center_x, center_y, w, h)形式的.
    https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py
    Args:
        xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)
    Returns: 
        bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)
    """
    return tf.concat(((xy[:, 2:] + xy[:, :2]) / 2,  #c_x, c_y
              xy[:, 2:] - xy[:, :2]), axis=1)

```


```python
def MultiBoxTarget(anchor, label):
    """
    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成归一化(xmin, ymin, xmax, ymax).
    https://zh.d2l.ai/chapter_computer-vision/anchor.html
    Args:
        anchor: torch tensor, 输入的锚框, 一般是通过MultiBoxPrior生成, shape:（1，锚框总数，4）
        label: 真实标签, shape为(bn, 每张图片最多的真实锚框数, 5)
               第二维中，如果给定图片没有这么多锚框, 可以先用-1填充空白, 最后一维中的元素为[类别标签, 四个坐标值]
    Returns:
        列表, [bbox_offset, bbox_mask, cls_labels]
        bbox_offset: 每个锚框的标注偏移量，形状为(bn，锚框总数*4)
        bbox_mask: 形状同bbox_offset, 每个锚框的掩码, 一一对应上面的偏移量, 负类锚框(背景)对应的掩码均为0, 正类锚框的掩码均为1
        cls_labels: 每个锚框的标注类别, 其中0表示为背景, 形状为(bn，锚框总数)
    """
    assert len(anchor.shape) == 3 and len(label.shape) == 3
    bn = label.shape[0]

    def MultiBoxTarget_one(anchor, label, eps=1e-6):
        """
        MultiBoxTarget函数的辅助函数, 处理batch中的一个
        Args:
            anchor: shape of (锚框总数, 4)
            label: shape of (真实锚框数, 5), 5代表[类别标签, 四个坐标值]
            eps: 一个极小值, 防止log0
        Returns:
            offset: (锚框总数*4, )
            bbox_mask: (锚框总数*4, ), 0代表背景, 1代表非背景
            cls_labels: (锚框总数, 4), 0代表背景
        """
        an = anchor.shape[0]
        assigned_idx = assign_anchor(label[:, 1:], anchor) ## (锚框总数, )
        # 决定anchor留下或者舍去
        bbox_mask = tf.repeat(tf.expand_dims(tf.cast((assigned_idx >= 0), dtype=tf.double), axis=-1), repeats=4, axis=1)

        cls_labels = np.zeros(an, dtype=int) # 0表示背景
        assigned_bb = np.zeros((an, 4), dtype=float) # 所有anchor对应的bb坐标
        for i in range(an):
            bb_idx = assigned_idx[i]
            if bb_idx >= 0: # 即非背景
                cls_labels[i] = label.numpy()[bb_idx, 0] + 1 # 要注意加1
                assigned_bb[i, :] = label.numpy()[bb_idx, 1:]
        
        center_anchor = tf.cast(xy_to_cxcy(anchor), dtype=tf.double)  # (center_x, center_y, w, h)
        center_assigned_bb = tf.cast(xy_to_cxcy(assigned_bb), dtype=tf.double) # (center_x, center_y, w, h)

        offset_xy = 10.0 * (center_assigned_bb[:,:2] - center_anchor[:,:2]) / center_anchor[:,2:]
        offset_wh = 5.0 * tf.math.log(eps + center_assigned_bb[:, 2:] / center_anchor[:, 2:])
        offset = tf.multiply(tf.concat((offset_xy, offset_wh), axis=1), bbox_mask)    # (锚框总数, 4)

        return tf.reshape(offset, (-1,)), tf.reshape(bbox_mask, (-1,)), cls_labels
    
    batch_offset = []
    batch_mask = []
    batch_cls_labels = []
    for b in range(bn):
        offset, bbox_mask, cls_labels = MultiBoxTarget_one(anchor[0, :, :], label[b,:,:])

        batch_offset.append(offset)
        batch_mask.append(bbox_mask)
        batch_cls_labels.append(cls_labels)
    
    batch_offset = tf.convert_to_tensor(batch_offset)
    batch_mask = tf.convert_to_tensor(batch_mask)
    batch_cls_labels = tf.convert_to_tensor(batch_cls_labels)

    return [batch_offset, batch_mask, batch_cls_labels]
```

我们通过tf.expand_dims函数为锚框和真实边界框添加样本维。


```python
labels = MultiBoxTarget(tf.expand_dims(anchors, axis=0), 
            tf.expand_dims(ground_truth, axis=0))
```


```python
labels[2]
```




    <tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[0, 1, 2, 0, 0]])>



我们根据锚框与真实边界框在图像中的位置来分析这些标注的类别。首先，在所有的“锚框—真实边界框”的配对中，锚框 A4 与猫的真实边界框的交并比最大，因此锚框 A4 的类别标注为猫。不考虑锚框 A4 或猫的真实边界框，在剩余的“锚框—真实边界框”的配对中，最大交并比的配对为锚框 A1 和狗的真实边界框，因此锚框 A1 的类别标注为狗。接下来遍历未标注的剩余3个锚框：与锚框 A0 交并比最大的真实边界框的类别为狗，但交并比小于阈值（默认为0.5），因此类别标注为背景；与锚框 A2 交并比最大的真实边界框的类别为猫，且交并比大于阈值，因此类别标注为猫；与锚框 A3 交并比最大的真实边界框的类别为猫，但交并比小于阈值，因此类别标注为背景。

返回值的第二项为掩码（mask）变量，形状为(批量大小, 锚框个数的四倍)。掩码变量中的元素与每个锚框的4个偏移量一一对应。 由于我们不关心对背景的检测，有关负类的偏移量不应影响目标函数。通过按元素乘法，掩码变量中的0可以在计算目标函数之前过滤掉负类的偏移量。


```python
labels[1]
```




    <tf.Tensor: shape=(1, 20), dtype=float64, numpy=
    array([[0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
            0., 0., 0., 0.]])>



返回的第一项是为每个锚框标注的四个偏移量，其中负类锚框的偏移量标注为0。


```python
labels[0]
```




    <tf.Tensor: shape=(1, 20), dtype=float64, numpy=
    array([[-0.        , -0.        , -0.        , -0.        ,  0.99999964,
             9.99999925,  1.53742723,  8.04719049, -4.00000036,  0.64516147,
             1.96021348, -0.44973579, -0.        , -0.        , -0.        ,
            -0.        , -0.        , -0.        , -0.        , -0.        ]])>



## 9.4.4. 输出预测边界框

在模型预测阶段，我们先为图像生成多个锚框，并为这些锚框一一预测类别和偏移量。随后，我们根据锚框及其预测偏移量得到预测边界框。当锚框数量较多时，同一个目标上可能会输出较多相似的预测边界框。为了使结果更加简洁，我们可以移除相似的预测边界框。常用的方法叫作非极大值抑制（non-maximum suppression，NMS）。

我们来描述一下非极大值抑制的工作原理。对于一个预测边界框 B ，模型会计算各个类别的预测概率。设其中最大的预测概率为 p ，该概率所对应的类别即 B 的预测类别。我们也将 p 称为预测边界框 B 的置信度。在同一图像上，我们将预测类别非背景的预测边界框按置信度从高到低排序，得到列表 L 。从 L 中选取置信度最高的预测边界框 B1 作为基准，将所有与 B1 的交并比大于某阈值的非基准预测边界框从 L 中移除。这里的阈值是预先设定的超参数。此时， L 保留了置信度最高的预测边界框并移除了与其相似的其他预测边界框。 接下来，从 L 中选取置信度第二高的预测边界框 B2 作为基准，将所有与 B2 的交并比大于某阈值的非基准预测边界框从 L 中移除。重复这一过程，直到 L 中所有的预测边界框都曾作为基准。此时 L 中任意一对预测边界框的交并比都小于阈值。最终，输出列表 L 中的所有预测边界框。




下面来看一个具体的例子。先构造4个锚框。简单起见，我们假设预测偏移量全是0：预测边界框即锚框。最后，我们构造每个类别的预测概率。


```python
anchors = tf.convert_to_tensor([[0.1, 0.08, 0.52, 0.92],
                [0.08, 0.2, 0.56, 0.95],
                [0/15, 0.3, 0.62, 0.91],
                [0.55, 0.2, 0.9, 0.88]])
offset_preds = tf.convert_to_tensor([0.0] * (4 * len(anchors)))
cls_probs = tf.convert_to_tensor([[0., 0., 0., 0.], # 背景的预测概率
                [0.9, 0.8, 0.7, 0.1],    # 狗的预测概率
                [0.1, 0.2, 0.3, 0.9]])   # 猫的预测概率
anchors, offset_preds, cls_probs
```




    (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
     array([[0.1 , 0.08, 0.52, 0.92],
            [0.08, 0.2 , 0.56, 0.95],
            [0.  , 0.3 , 0.62, 0.91],
            [0.55, 0.2 , 0.9 , 0.88]], dtype=float32)>,
     <tf.Tensor: shape=(16,), dtype=float32, numpy=
     array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
           dtype=float32)>,
     <tf.Tensor: shape=(3, 4), dtype=float32, numpy=
     array([[0. , 0. , 0. , 0. ],
            [0.9, 0.8, 0.7, 0.1],
            [0.1, 0.2, 0.3, 0.9]], dtype=float32)>)



在图像上打印预测边界框和它们的置信度。


```python
fig = plt.imshow(img)
show_bboxes(fig.axes, anchors * bbox_scale,
        ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])
```


![svg](../img/chapter09/9.4/output_56_0.svg)


下面我们实现MultiBoxDetection函数来执行非极大值抑制。


```python
from collections import namedtuple
Pred_BB_Info = namedtuple("Pred_BB_Info", 
        ["index", "class_id", "confidence", "xyxy"])

def non_max_suppression(bb_info_list, nms_threshold=0.5):
    """
    非极大抑制处理预测的边界框
    Args:
        bb_info_list: Pred_BB_Info的列表, 包含预测类别、置信度等信息
        nms_threshold: 阈值
    Returns:
        output: Pred_BB_Info的列表, 只保留过滤后的边界框信息
    """
    output = []
    # 现根据置信度从高到底排序
    sorted_bb_info_list = sorted(bb_info_list,
                    key = lambda x: x.confidence, 
                    reverse=True)
    while len(sorted_bb_info_list) != 0:
        best = sorted_bb_info_list.pop(0)
        output.append(best)

        if len(sorted_bb_info_list) == 0:
            break
        bb_xyxy = []
        for bb in sorted_bb_info_list:
            bb_xyxy.append(bb.xyxy)
        
        iou = compute_jaccard(tf.convert_to_tensor(best.xyxy),
                    tf.squeeze(tf.convert_to_tensor(bb_xyxy), axis=1))[0] # shape: (len(sorted_bb_info_list), )
        n = len(sorted_bb_info_list)
        sorted_bb_info_list = [
                    sorted_bb_info_list[i] for i in 
                    range(n) if iou[i] <= nms_threshold]
    return output
```


```python
def MultiBoxDetection(cls_prob, loc_pred, anchor, nms_threshold=0.5):
    """
    # 按照「9.4.1. 生成多个锚框」所讲的实现, anchor表示成归一化(xmin, ymin, xmax, ymax).
    https://zh.d2l.ai/chapter_computer-vision/anchor.html
    Args:
        cls_prob: 经过softmax后得到的各个锚框的预测概率, shape:(bn, 预测总类别数+1, 锚框个数)
        loc_pred: 预测的各个锚框的偏移量, shape:(bn, 锚框个数*4)
        anchor: MultiBoxPrior输出的默认锚框, shape: (1, 锚框个数, 4)
        nms_threshold: 非极大抑制中的阈值
    Returns:
        所有锚框的信息, shape: (bn, 锚框个数, 6)
        每个锚框信息由[class_id, confidence, xmin, ymin, xmax, ymax]表示
        class_id=-1 表示背景或在非极大值抑制中被移除了
    """
    assert len(cls_prob.shape) == 3 and len(loc_pred.shape) == 2 and len(anchor.shape) == 3
    bn = cls_prob.shape[0]

    def MultiBoxDetection_one(c_p, l_p, anc, nms_threshold=0.5):
        """
        MultiBoxDetection的辅助函数, 处理batch中的一个
        Args:
            c_p: (预测总类别数+1, 锚框个数)
            l_p: (锚框个数*4, )
            anc: (锚框个数, 4)
            nms_threshold: 非极大抑制中的阈值
        Return:
            output: (锚框个数, 6)
        """
        pred_bb_num = c_p.shape[1]
        # 加上偏移量
        anc = tf.add(anc, tf.reshape(l_p, (pred_bb_num, 4))).numpy()

        # 最大的概率
        confidence = tf.reduce_max(c_p, axis=0)
        # 最大概率对应的id
        class_id = tf.argmax(c_p, axis=0)
        confidence = confidence.numpy()
        class_id = class_id.numpy()

        pred_bb_info = [Pred_BB_Info(index=i,
                    class_id=class_id[i]-1,
                    confidence=confidence[i],
                    xyxy=[anc[i]]) # xyxy是个列表
                for i in range(pred_bb_num)]
        # 正类的index
        obj_bb_idx = [bb.index for bb 
                in non_max_suppression(pred_bb_info,
                            nms_threshold)]
        output = []
        for bb in pred_bb_info:
            output.append(np.append([
                (bb.class_id if bb.index in obj_bb_idx 
                        else -1.0),
                bb.confidence],
                bb.xyxy))
        
        return tf.convert_to_tensor(output) # shape: (锚框个数， 6)
    
    batch_output = []
    for b in range(bn):
        batch_output.append(MultiBoxDetection_one(cls_prob[b],
                        loc_pred[b], anchor[0],
                        nms_threshold))
    
    return tf.convert_to_tensor(batch_output)
```

然后我们运行MultiBoxDetection函数并设阈值为0.5。这里为输入都增加了样本维。我们看到，返回的结果的形状为(批量大小, 锚框个数, 6)。其中每一行的6个元素代表同一个预测边界框的输出信息。第一个元素是索引从0开始计数的预测类别（0为狗，1为猫），其中-1表示背景或在非极大值抑制中被移除。第二个元素是预测边界框的置信度。剩余的4个元素分别是预测边界框左上角的xx和yy轴坐标以及右下角的xx和yy轴坐标（值域在0到1之间）。


```python
output = MultiBoxDetection(
    tf.expand_dims(cls_probs, 0),
    tf.expand_dims(offset_preds, 0),
    tf.expand_dims(anchors, 0),
    nms_threshold=0.5)
output
```




    <tf.Tensor: shape=(1, 4, 6), dtype=float64, numpy=
    array([[[ 0.        ,  0.89999998,  0.1       ,  0.08      ,
              0.51999998,  0.92000002],
            [-1.        ,  0.80000001,  0.08      ,  0.2       ,
              0.56      ,  0.94999999],
            [-1.        ,  0.69999999,  0.        ,  0.30000001,
              0.62      ,  0.91000003],
            [ 1.        ,  0.89999998,  0.55000001,  0.2       ,
              0.89999998,  0.88      ]]])>



我们移除掉类别为-1的预测边界框，并可视化非极大值抑制保留的结果。


```python
fig = plt.imshow(img)
for i in output[0].numpy():
    if i[0] == -1:
        continue
    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])
    show_bboxes(fig.axes, tf.multiply(i[2:], bbox_scale), label)
```


![svg](../img/chapter09/9.4/output_63_0.svg)


实践中，我们可以在执行非极大值抑制前将置信度较低的预测边界框移除，从而减小非极大值抑制的计算量。我们还可以筛选非极大值抑制的输出，例如，只保留其中置信度较高的结果作为最终输出。

## 小结

* 以每个像素为中心，生成多个大小和宽高比不同的锚框。
* 交并比是两个边界框相交面积与相并面积之比。
* 在训练集中，为每个锚框标注两类标签：一是锚框所含目标的类别；二是真实边界框相对锚框的偏移量。
* 预测时，可以使用非极大值抑制来移除相似的预测边界框，从而令结果简洁。
